{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "## [1. Tensorflow_VGG16](#Tensorflow_VGG16)\n",
    "## [2. Custom_VGG16](#Custom_VGG16)\n",
    "## [3. Style Transfer Network](#Style_Transfer_Network)\n",
    "## [4. Train](#Train)\n",
    "## [5. Freeze Graph](#Freeze_Graph)\n",
    "## [6. Generate Image](#Generate_Image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow_VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vgg16.png\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "class Vgg16:\n",
    "    \n",
    "    def __init__(self, vgg16_npy_path=None):\n",
    "        # VGG16 모델의 W를 배열 형태로 불러옴\n",
    "        if vgg16_npy_path is None:\n",
    "            path = inspect.getfile(Vgg16)\n",
    "            path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "            path = os.path.join(path, \"vgg16.npy\")\n",
    "            vgg16_npy_path = path\n",
    "            print(path)\n",
    "            \n",
    "        self.data_dict = np.load(vgg16_npy_path, encoding='latin1').item()\n",
    "        print(\"npy file loaded\")\n",
    "\n",
    "    def build(self, rgb):\n",
    "        \n",
    "        \"\"\"\n",
    "        load variable from npy to build the VGG\n",
    "        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(\"build model started\")\n",
    "        rgb_scaled = rgb * 255.0\n",
    "\n",
    "        # RGB를 BGR로 \n",
    "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=rgb_scaled)\n",
    "        assert red.get_shape().as_list()[1:] == [224, 224, 1]\n",
    "        assert green.get_shape().as_list()[1:] == [224, 224, 1]\n",
    "        assert blue.get_shape().as_list()[1:] == [224, 224, 1]\n",
    "\n",
    "        bgr = tf.concat(axis=3, values=[\n",
    "            blue - VGG_MEAN[0],\n",
    "            green - VGG_MEAN[1],\n",
    "            red - VGG_MEAN[2],\n",
    "        ])\n",
    "        # vgg_mean을 빼서 pixel 값들을 0이 중심이 되게 바꿈\n",
    "\n",
    "        # 이미지는 224*224*3의 형태\n",
    "        assert bgr.get_shape().as_list()[1:] == [224, 224, 3]\n",
    "\n",
    "        self.conv1_1 = self.conv_layer(bgr, \"conv1_1\")\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\n",
    "        self.pool1 = self.max_pool(self.conv1_2, 'pool1')\n",
    "        # convolution 은 3*3의 filter를 사용\n",
    "        # pooling 은 2*2의 kernel을 사용\n",
    "\n",
    "        self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\n",
    "        self.pool2 = self.max_pool(self.conv2_2, 'pool2')\n",
    "\n",
    "        self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n",
    "        self.pool3 = self.max_pool(self.conv3_3, 'pool3')\n",
    "\n",
    "        self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n",
    "        self.pool4 = self.max_pool(self.conv4_3, 'pool4')\n",
    "\n",
    "        self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n",
    "        self.pool5 = self.max_pool(self.conv5_3, 'pool5')\n",
    "\n",
    "        self.fc6 = self.fc_layer(self.pool5, \"fc6\")\n",
    "        assert self.fc6.get_shape().as_list()[1:] == [4096]\n",
    "        # fully connected layer를 통해 길이가 4096인 벡터로 변환\n",
    "        self.relu6 = tf.nn.relu(self.fc6)\n",
    "\n",
    "        self.fc7 = self.fc_layer(self.relu6, \"fc7\")\n",
    "        self.relu7 = tf.nn.relu(self.fc7)\n",
    "\n",
    "        self.fc8 = self.fc_layer(self.relu7, \"fc8\")\n",
    "        self.prob = tf.nn.softmax(self.fc8, name=\"prob\")\n",
    "\n",
    "        self.data_dict = None\n",
    "        print((\"build model finished: %ds\" % (time.time() - start_time)))\n",
    "\n",
    "    def avg_pool(self, bottom, name):\n",
    "        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool(self, bottom, name):\n",
    "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def conv_layer(self, bottom, name):\n",
    "        with tf.variable_scope(name):\n",
    "            filt = self.get_conv_filter(name)\n",
    "            conv = tf.nn.conv2d(bottom, filt, [1, 1, 1, 1], padding='SAME')\n",
    "            conv_biases = self.get_bias(name)\n",
    "            bias = tf.nn.bias_add(conv, conv_biases)\n",
    "            relu = tf.nn.relu(bias)\n",
    "            return relu\n",
    "\n",
    "    def fc_layer(self, bottom, name):\n",
    "        with tf.variable_scope(name):\n",
    "            shape = bottom.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(bottom, [-1, dim])\n",
    "            weights = self.get_fc_weight(name)\n",
    "            biases = self.get_bias(name)\n",
    "            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n",
    "            return fc\n",
    "\n",
    "    def get_conv_filter(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name=\"filter\")\n",
    "\n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name=\"biases\")\n",
    "\n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name=\"weights\")\n",
    "    # 미리 학습된 VGG16의 파라미터들을 가져와 constant에 할당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom_VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre train된 VGG16 모델을 불러와서 몇몇 layer만 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, inspect\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow_vgg import vgg16\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68]\n",
    "\n",
    "def loadWeightsData(vgg16_npy_path=None):\n",
    "    # VGG16 모델을 불러옴\n",
    "    if vgg16_npy_path is None:\n",
    "        path = inspect.getfile(Vgg16)\n",
    "        path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "        path = os.path.join(path, \"vgg16.npy\")\n",
    "        vgg16_npy_path = path\n",
    "        print (vgg16_npy_path)\n",
    "    return np.load(vgg16_npy_path, encoding='latin1').item()\n",
    "\n",
    "class custom_Vgg16(vgg16.Vgg16):\n",
    "    # 객체 생성시 input으로 [batch, height, width, 3] 형태의 tensor를 받음\n",
    "    # values scaled [0, 1]\n",
    "    # Tensorflow_VGG16을 상속받음\n",
    "    # 객체 생성시에 input을 받아 layer들을 미리 만들어둠\n",
    "    \n",
    "    def __init__(self, rgb, data_dict, train=False):\n",
    "        self.data_dict = data_dict\n",
    "\n",
    "        # start_time = time.time()\n",
    "        # rgb_scaled = rgb * 255.0\n",
    "\n",
    "        rgb_scaled = rgb\n",
    "\n",
    "        red, green, blue = tf.split(rgb_scaled, 3, 3)\n",
    "        bgr = tf.concat([blue - VGG_MEAN[0],\n",
    "                        green - VGG_MEAN[1],\n",
    "                        red - VGG_MEAN[2]],\n",
    "                        3)\n",
    "\n",
    "        self.conv1_1 = self.conv_layer(bgr, \"conv1_1\")\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, \"conv1_2\")\n",
    "        self.pool1 = self.max_pool(self.conv1_2, 'pool1')\n",
    "\n",
    "\n",
    "        self.conv2_1 = self.conv_layer(self.pool1, \"conv2_1\")\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, \"conv2_2\")\n",
    "        self.pool2 = self.max_pool(self.conv2_2, 'pool2')\n",
    "\n",
    "\n",
    "        self.conv3_1 = self.conv_layer(self.pool2, \"conv3_1\")\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, \"conv3_2\")\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, \"conv3_3\")\n",
    "        self.pool3 = self.max_pool(self.conv3_3, 'pool3')\n",
    "\n",
    "\n",
    "        self.conv4_1 = self.conv_layer(self.pool3, \"conv4_1\")\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, \"conv4_2\")\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, \"conv4_3\")\n",
    "        self.pool4 = self.max_pool(self.conv4_3, 'pool4')\n",
    "\n",
    "\n",
    "        self.conv5_1 = self.conv_layer(self.pool4, \"conv5_1\")\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, \"conv5_2\")\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, \"conv5_3\")\n",
    "        self.pool5 = self.max_pool(self.conv5_3, 'pool5')\n",
    "        #fully connected layer 전 까지의 layer들만 사용함\n",
    "\n",
    "        # self.data_dict = None\n",
    "        # print (\"build model finished: %ds\" % (time.time() - start_time))\n",
    "\n",
    "    def debug(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style_Transfer_Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "<img src=\"batchnorm.png\" width=\"300px\">\n",
    "$$\\gamma : scale$$\n",
    "$$\\beta : offset$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    # input으로 넣은 shape의 모양으로 W를 truncated normal 분포로 초기화하는 함수 (잘린 normal 분포)\n",
    "    initial = tf.truncated_normal(shape, stddev=0.001)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def conv2d(x, W, strides=[1, 1, 1, 1], p='SAME', name=None):\n",
    "    # 2D convolution을 만드는 함수\n",
    "    assert isinstance(x, tf.Tensor)\n",
    "    return tf.nn.conv2d(x, W, strides=strides, padding=p, name=name)\n",
    "\n",
    "def batch_norm(x):\n",
    "    assert isinstance(x, tf.Tensor)\n",
    "    # reduce dimension 1, 2, 3, which would produce batch mean and batch variance.\n",
    "    mean, var = tf.nn.moments(x, axes=[1, 2, 3])\n",
    "    # input x의 mean과 variance을 구함\n",
    "    return tf.nn.batch_normalization(x, mean, var, 0, 1, 1e-5)\n",
    "    # offset=0, scale=1, variance_epsilon=0.00001로 batch normalization 적용\n",
    "\n",
    "def relu(x):\n",
    "    assert isinstance(x, tf.Tensor)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def deconv2d(x, W, strides=[1, 1, 1, 1], p='SAME', name=None):\n",
    "    assert isinstance(x, tf.Tensor)\n",
    "    _, _, c, _ = W.get_shape().as_list()ㄹ\n",
    "    b, h, w, _ = x.get_shape().as_list()\n",
    "    return tf.nn.conv2d_transpose(x, W, [b, strides[1] * h, strides[1] * w, c], strides=strides, padding=p, name=name)\n",
    "    # 2D convolution을 역으로 적용하는 함수\n",
    "    # [batch, 224, 224, channel]의 형태로 변경\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    assert isinstance(x, tf.Tensor)\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    # 2*2 크기로 max pooling을 적용하는 함수\n",
    "\n",
    "class ResidualBlock():\n",
    "\n",
    "    def __init__(self, idx, ksize=3, train=False, data_dict=None):\n",
    "        self.W1 = weight_variable([ksize, ksize, 128, 128], name='R'+ str(idx) + '_conv1_w')\n",
    "        self.W2 = weight_variable([ksize, ksize, 128, 128], name='R'+ str(idx) + '_conv2_w')\n",
    "        # 3*3*128*128의 크기의 W를 설정\n",
    "    def __call__(self, x, idx, strides=[1, 1, 1, 1]):\n",
    "        h = relu(batch_norm(conv2d(x, self.W1, strides, name='R' + str(idx) + '_conv1')))\n",
    "        # 3*3*128*128의 크기로 2D convolution을 적용함\n",
    "        h = batch_norm(conv2d(h, self.W2, name='R' + str(idx) + '_conv2'))\n",
    "        # batch normalization을 적용\n",
    "        return x + h\n",
    "        # Residual Block의 input과 input을 두번의 Convolution을 적용한 결과를 더함\n",
    "\n",
    "\n",
    "\n",
    "class FastStyleNet():\n",
    "    def __init__(self, train=True, data_dict=None):\n",
    "        self.c1 = weight_variable([9, 9, 3, 32], name='t_conv1_w')\n",
    "        self.c2 = weight_variable([4, 4, 32, 64], name='t_conv2_w')\n",
    "        self.c3 = weight_variable([4, 4, 64, 128], name='t_conv3_w')\n",
    "        self.r1 = ResidualBlock(1, train=train)\n",
    "        self.r2 = ResidualBlock(2, train=train)\n",
    "        self.r3 = ResidualBlock(3, train=train)\n",
    "        self.r4 = ResidualBlock(4, train=train)\n",
    "        self.r5 = ResidualBlock(5, train=train)\n",
    "        self.d1 = weight_variable([4, 4, 64, 128], name='t_dconv1_w')\n",
    "        self.d2 = weight_variable([4, 4, 32, 64], name='t_dconv2_w')\n",
    "        self.d3 = weight_variable([9, 9, 3, 32], name='t_dconv3_w')            \n",
    "        # 각각의 W들과 Residual Block을 초기화\n",
    "        \n",
    "    def __call__(self, h):\n",
    "        h = batch_norm(relu(conv2d(h, self.c1, name='t_conv1')))\n",
    "        # [batch, 224, 224, 32]\n",
    "        h = batch_norm(relu(conv2d(h, self.c2, strides=[1, 2, 2, 1], name='t_conv2')))\n",
    "        # 이미지의 크기를 절반으로 줄임 [batch, 112, 112, 64]\n",
    "        h = batch_norm(relu(conv2d(h, self.c3, strides=[1, 2, 2, 1], name='t_conv3')))\n",
    "        # [batch, 56, 56, 128]\n",
    "        # input으로 image를 받으면 3번의 convolution을 거침\n",
    "        \n",
    "        h = self.r1(h, 1)\n",
    "        h = self.r2(h, 2)\n",
    "        h = self.r3(h, 3)\n",
    "        h = self.r4(h, 4)\n",
    "        h = self.r5(h, 5)\n",
    "        # [batch, 56, 56, 128]\n",
    "        # convolution을 통해 얻은 결과를 5개의 residual block에 넣여 연산을 함\n",
    "        \n",
    "        h = batch_norm(relu(deconv2d(h, self.d1, strides=[1, 2, 2, 1], name='t_deconv1')))\n",
    "        # 이미지의 크기를 두배로 늘림 [batch, 112, 112, 64]\n",
    "        h = batch_norm(relu(deconv2d(h, self.d2, strides=[1, 2, 2, 1], name='t_deconv2')))\n",
    "        # [batch, 224, 224, 32]\n",
    "        y = deconv2d(h, self.d3, name='t_deconv3')\n",
    "        # residual block을 통해 얻은 결과를 3개의 deconvolution을 거쳐 다시 [batch, 224, 224, 3]의 형태로 바꿈\n",
    "        \n",
    "        return tf.multiply((tf.tanh(y) + 1), tf.constant(127.5, tf.float32, shape=y.get_shape()), name='output')\n",
    "        # 왜 127.5??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gram matrix\n",
    "<img src=\"gram.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from freeze_graph import freeze_graph\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from net import *\n",
    "sys.path.append(os.path.join(os.path.dirname(sys.path[0]), \"./\"))\n",
    "from custom_vgg16 import *\n",
    "\n",
    "\n",
    "def gram_matrix(x):\n",
    "    # gram matrix를 만드는 함수\n",
    "    assert isinstance(x, tf.Tensor)\n",
    "    b, h, w, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [b, h*w, ch])\n",
    "    # gram = tf.batch_matmul(features, features, adj_x=True)/tf.constant(ch*w*h, tf.float32)\n",
    "    gram = tf.matmul(features, features, adjoint_a=True)/tf.constant(ch*w*h, tf.float32)\n",
    "    # adjoint_a 옵션을 줘서 features.T 와 features의 matmul을 구함\n",
    "    # gram matrix를 통해 각 channel간 correlation을 구함\n",
    "    return gram\n",
    "\n",
    "# total variation denoising\n",
    "\n",
    "def total_variation_regularization(x, beta=1):\n",
    "    assert isinstance(x, tf.Tensor)\n",
    "    wh = tf.constant([[[[ 1], [ 1], [ 1]]], [[[-1], [-1], [-1]]]], tf.float32)\n",
    "    ww = tf.constant([[[[ 1], [ 1], [ 1]], [[-1], [-1], [-1]]]], tf.float32)\n",
    "    tvh = lambda x: conv2d(x, wh, p='SAME')\n",
    "    tvw = lambda x: conv2d(x, ww, p='SAME')\n",
    "    dh = tvh(x)\n",
    "    dw = tvw(x)\n",
    "    tv = (tf.add(tf.reduce_sum(dh**2, [1, 2, 3]), tf.reduce_sum(dw**2, [1, 2, 3]))) ** (beta / 2.)\n",
    "    return tv\n",
    "\n",
    "\n",
    "# Argument parsing\n",
    "parser = argparse.ArgumentParser(description='Real-time style transfer')\n",
    "parser.add_argument('--gpu', '-g', default=-1, type=int,\n",
    "                    help='GPU ID (negative value indicates CPU)')\n",
    "parser.add_argument('--dataset', '-d', default='dataset', type=str,\n",
    "                    help='dataset directory path (according to the paper, use MSCOCO 80k images)')\n",
    "parser.add_argument('--style_image', '-s', type=str, required=True,\n",
    "                    help='style image path')\n",
    "parser.add_argument('--batchsize', '-b', type=int, default=1,\n",
    "                    help='batch size (default value is 1)')\n",
    "parser.add_argument('--ckpt', '-c', default=None, type=int,\n",
    "                    help='the global step of checkpoint file desired to restore.')\n",
    "parser.add_argument('--lambda_tv', '-l_tv', default=10e-4, type=float,\n",
    "                    help='weight of total variation regularization according to the paper to be set between 10e-4 and 10e-6.')\n",
    "parser.add_argument('--lambda_feat', '-l_feat', default=1e0, type=float)\n",
    "parser.add_argument('--lambda_style', '-l_style', default=1e1, type=float)\n",
    "parser.add_argument('--epoch', '-e', default=2, type=int)\n",
    "parser.add_argument('--lr', '-l', default=1e-3, type=float)\n",
    "parser.add_argument('--pb', '-pb', default=True, type=bool, help='save a pb format as well.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_dict = loadWeightsData('./vgg16.npy')\n",
    "# pre train된 VGG16을 불러옴\n",
    "\n",
    "batchsize = args.batchsize\n",
    "gpu = args.gpu\n",
    "dataset = args.dataset\n",
    "epochs = args.epoch\n",
    "learning_rate = args.lr\n",
    "ckpt = args.ckpt\n",
    "lambda_tv = args.lambda_tv\n",
    "lambda_f = args.lambda_feat\n",
    "lambda_s = args.lambda_style\n",
    "style_image = args.style_image\n",
    "save_pb = args.pb\n",
    "gpu = args.gpu\n",
    "#Argument를 설정\n",
    "\n",
    "style_name, _ = os.path.splitext(style_image.split(os.sep)[-1])\n",
    "fpath = os.listdir(args.dataset)\n",
    "imagepaths = []\n",
    "for fn in fpath:\n",
    "    base, ext = os.path.splitext(fn)\n",
    "    if ext == '.jpg' or ext == '.png':\n",
    "        imagepath = os.path.join(dataset, fn)\n",
    "        imagepaths.append(imagepath)\n",
    "data_len = len(imagepaths)\n",
    "iterations = int(data_len / batchsize)\n",
    "print ('Number of traning images: {}'.format(data_len))\n",
    "print ('{} epochs, {} iterations per epoch'.format(epochs, iterations))\n",
    "#이미지 path를 설정\n",
    "\n",
    "\n",
    "style_np = np.asarray(Image.open(style_image).convert('RGB').resize((224, 224)), dtype=np.float32)\n",
    "styles_np = [style_np for x in range(batchsize)]\n",
    "\n",
    "\n",
    "if gpu > -1:\n",
    "    device = '/gpu:{}'.format(gpu)\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "    \n",
    "    \n",
    "with tf.device(device):\n",
    "    inputs = tf.placeholder(tf.float32, shape=[batchsize, 224, 224, 3], name='input')\n",
    "    # input으로 받을 placeholder를 설정\n",
    "    net = FastStyleNet()\n",
    "    saver = tf.train.Saver(restore_sequentially=True)\n",
    "    # 모델을 저장하기 위한 Saver 객체를 생성\n",
    "    saver_def = saver.as_saver_def()\n",
    "    # 왜 정의?\n",
    "    \n",
    "\n",
    "    target = tf.placeholder(tf.float32, shape=[batchsize, 224, 224, 3])\n",
    "    # target 이미지를 받을 placeholder를 설정 (style target)\n",
    "    outputs = net(inputs)\n",
    "\n",
    "\n",
    "    # style target feature\n",
    "    # compute gram maxtrix of style target\n",
    "\n",
    "    vgg_s = custom_Vgg16(target, data_dict=data_dict)\n",
    "    # style target feature를 구하기 위한 VGG16을 만듬\n",
    "    feature_ = [vgg_s.conv1_2, vgg_s.conv2_2, vgg_s.conv3_3, vgg_s.conv4_3, vgg_s.conv5_3]\n",
    "    gram_ = [gram_matrix(l) for l in feature_]\n",
    "\n",
    "    vgg_c = custom_Vgg16(inputs, data_dict=data_dict)\n",
    "    # content target feature를 구하기 위한 VGG16을 만듬\n",
    "    feature_ = [vgg_c.conv1_2, vgg_c.conv2_2, vgg_c.conv3_3, vgg_c.conv4_3, vgg_c.conv5_3]\n",
    "\n",
    "    vgg = custom_Vgg16(outputs, data_dict=data_dict)\n",
    "    # output의 feature를 구하기 위한 VGG16을 만듬\n",
    "    feature = [vgg.conv1_2, vgg.conv2_2, vgg.conv3_3, vgg.conv4_3, vgg.conv5_3]\n",
    "\n",
    "    # compute feature loss\n",
    "    loss_f = tf.zeros(batchsize, tf.float32)\n",
    "    for f, f_ in zip(feature, feature_):\n",
    "        loss_f += lambda_f * tf.reduce_mean(tf.subtract(f, f_) ** 2, [1, 2, 3])\n",
    "        # output과 content target(x와 같음)의 feature로 loss를 구함\n",
    "\n",
    "    # compute style loss\n",
    "    gram = [gram_matrix(l) for l in feature]\n",
    "    loss_s = tf.zeros(batchsize, tf.float32)\n",
    "    for g, g_ in zip(gram, gram_):\n",
    "        loss_s += lambda_s * tf.reduce_mean(tf.subtract(g, g_) ** 2, [1, 2])\n",
    "        # output과 style target의 gram matrix를 각각 구해 style loss를 구함\n",
    "\n",
    "    # total variation denoising\n",
    "    loss_tv = lambda_tv * total_variation_regularization(outputs)\n",
    "\n",
    "    # total loss\n",
    "    loss = loss_s + loss_f + loss_tv\n",
    "\n",
    "    # optimizer\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    # gpu가 없으면 자동으로 cpu를 할당함\n",
    "    ckpt_directory = './ckpts/{}/'.format(style_name)\n",
    "    if not os.path.exists(ckpt_directory):\n",
    "        # 저장된 데이터가 있으면 해당 경로를 지정하고 없으면 새로운 경로를 만듬\n",
    "        os.makedirs(ckpt_directory)\n",
    "\n",
    "    # training\n",
    "    tf.global_variables_initializer().run()\n",
    "    if ckpt:\n",
    "        # Argument로 check point를 지정해주면 그 check point를 불러오고 지정하지 않으면 default로 가장 최근의 check point를 불러옴\n",
    "        if ckpt < 0:\n",
    "            checkpoint = tf.train.get_checkpoint_state(ckpt_directory)\n",
    "            input_checkpoint = checkpoint.model_checkpoint_path\n",
    "        else:\n",
    "            input_checkpoint =  ckpt_directory + style_name + '-{}'.format(ckpt)\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        print ('Checkpoint {} restored.'.format(ckpt))\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # 학습을 시작\n",
    "        imgs = np.zeros((batchsize, 224, 224, 3), dtype=np.float32)\n",
    "        for i in range(iterations):\n",
    "            for j in range(batchsize):\n",
    "                p = imagepaths[i * batchsize + j]\n",
    "                imgs[j] = np.asarray(Image.open(p).convert('RGB').resize((224, 224)), np.float32)\n",
    "                # train data를 가져와 imgs에 [224,224,3]의 tensor로 할당\n",
    "            feed_dict = {inputs: imgs, target: styles_np}\n",
    "            loss_, _= sess.run([loss, train_step,], feed_dict=feed_dict)\n",
    "            # 한번의 batch만큼 데이터를 할당하고 학습을 시킴\n",
    "            print('[epoch {}/{}] batch {}/{}... loss: {}'.format(epoch, epochs, i + 1, iterations, loss_[0]))    \n",
    "        saver.save(sess, ckpt_directory + style_name, global_step=epoch)\n",
    "        # 한번의 epoch만큼 학습이 끝나고 check point를 갱신\n",
    "\n",
    "if save_pb:\n",
    "    # 그래프를 pb 파일로 저장\n",
    "    # pb 파일은 그래프와 파라미터를 한번에 저장\n",
    "    if not os.path.exists('./pbs'):\n",
    "        os.makedirs('./pbs')\n",
    "    freeze_graph(ckpt_directory, './pbs/{}.pb'.format(style_name), 'output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze_Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "dir = os.path.dirname(os.path.realpath(__file__))\n",
    "# 그래프를 저장할 path를 지정\n",
    "\n",
    "def freeze_graph(model_folder, output_graph, output_node_name):\n",
    "    # 불러올 check point 경로를 설정\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_folder)\n",
    "    # 가장 최근의 checkpoint 정보를 가져옴\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    # 그 checkpoint의 실제 경로를 가져옴\n",
    "    \n",
    "    absolute_model_folder = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "    # 왜 정의?\n",
    "    # output_graph = absolute_model_folder + \"/{}.pb\".format(style_name)\n",
    "\n",
    "    # Before exporting our graph, we need to precise what is our output node\n",
    "    # This is how TF decides what part of the Graph he has to keep and what part it can dump\n",
    "    # NOTE: this variable is plural, because you can have multiple output nodes\n",
    "    output_node_names = output_node_name\n",
    "\n",
    "    # We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "    clear_devices = True\n",
    "\n",
    "    # We import the meta graph and retrieve a Saver\n",
    "    saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "    # 체크 포인트의 그래프 모양을 가져옴 => default graph가 불러온 그래프로 바뀜\n",
    "\n",
    "    # We retrieve the protobuf graph definition\n",
    "    graph = tf.get_default_graph()\n",
    "    # 가져온 그래프를 할당\n",
    "    input_graph_def = graph.as_graph_def()\n",
    "    # graph def를 할당함(그래프의 모양)\n",
    "#     node {\n",
    "#       name: \"x\"\n",
    "#       op: \"VariableV2\"\n",
    "#       attr {\n",
    "#         key: \"container\"\n",
    "#         value {\n",
    "#           s: \"\"\n",
    "#         }\n",
    "#       }\n",
    "#       attr {\n",
    "#         key: \"dtype\"\n",
    "#         value {\n",
    "#           type: DT_FLOAT\n",
    "#         }\n",
    "#       }\n",
    "#       attr {\n",
    "#         key: \"shape\"\n",
    "#         value {\n",
    "#           shape {\n",
    "#             dim {\n",
    "#               size: 3\n",
    "#             }\n",
    "#           }\n",
    "#         }\n",
    "#       }\n",
    "#       attr {\n",
    "#         key: \"shared_name\"\n",
    "#         value {\n",
    "#           s: \"\"\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "\n",
    "\n",
    "    # We start a session and restore the graph weights\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        # 불러온 체크포인를 세션에 복구시킴\n",
    "\n",
    "        # We use a built-in TF helper to export variables to constants\n",
    "        output_graph_def = graph_util.convert_variables_to_constants(\n",
    "            sess, # 파라미터 정보를 가지고 있음\n",
    "            input_graph_def, # 노드 정보를 가지고 있음\n",
    "            output_node_names.split(\",\") # 사용할 output 노드들의 이름\n",
    "        )\n",
    "# node {\n",
    "#   name: \"x\"\n",
    "#   op: \"Const\"\n",
    "#   attr {\n",
    "#     key: \"dtype\"\n",
    "#     value {\n",
    "#       type: DT_FLOAT\n",
    "#     }\n",
    "#   }\n",
    "#   attr {\n",
    "#     key: \"value\"\n",
    "#     value {\n",
    "#       tensor {\n",
    "#         dtype: DT_FLOAT\n",
    "#         tensor_shape {\n",
    "#           dim {\n",
    "#             size: 3\n",
    "#           }\n",
    "#         }\n",
    "#         tensor_content: \"\\221\\346\\323\\276\\036\\357\\326\\276\\337\\306\\224\\277\"\n",
    "#       }\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "        \n",
    "        \n",
    "        # Finally we serialize and dump the output graph to the filesystem\n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "            # output_graph_def를 byte 정보로 바꿔서 저장함\n",
    "        print(\"{} ops in the final graph.\".format(len(output_graph_def.node)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Real-time style transfer image generator')\n",
    "parser.add_argument('--input', '-i', type=str, help='content image')\n",
    "parser.add_argument('--gpu', '-g', default=-1, type=int,\n",
    "                    help='GPU ID (negative value indicates CPU)')\n",
    "parser.add_argument('--style', '-s', default=None, type=str, help='style model name')\n",
    "parser.add_argument('--ckpt', '-c', default=-1, type=int, help='checkpoint to be loaded')\n",
    "parser.add_argument('--out', '-o', default='stylized_image.jpg', type=str, help='stylized image\\'s name')\n",
    "parser.add_argument('--pb', '-pb', default=False, type=bool, help='load with pb')\n",
    "\n",
    "args = parser.parse_args()\n",
    "# Argument 파싱\n",
    "\n",
    "\n",
    "if not os.path.exists('./images/output/'):\n",
    "        os.makedirs('./images/output/')\n",
    "# output 이미지를 저장할 path를 지정\n",
    "        \n",
    "outfile_path = './images/output/' + args.out\n",
    "content_image_path = args.input\n",
    "style_name = args.style\n",
    "ckpt = args.ckpt\n",
    "load_with_pb = args.pb\n",
    "gpu = args.gpu\n",
    "\n",
    "original_image = Image.open(content_image_path).convert('RGB')\n",
    "# sytle을 변경할 original 이미지를 rgb 형태로 바꿈\n",
    "\n",
    "img = np.asarray(original_image.resize((224, 224)), dtype=np.float32)\n",
    "# 이미지를 224*224의 크기로 조정\n",
    "shaped_input = img.reshape((1,) + img.shape)\n",
    "# [1,224,224,3]의 형태로 변경 \n",
    "\n",
    "if gpu > -1:\n",
    "    device = '/gpu:{}'.format(gpu)\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "        if load_with_pb:\n",
    "            # 저장된 pb 파일을 불러옴\n",
    "            from tensorflow.core.framework import graph_pb2\n",
    "            graph_def = graph_pb2.GraphDef()\n",
    "            with open('./pbs/{}.pb'.format(style_name), \"rb\") as f:\n",
    "                graph_def.ParseFromString(f.read())\n",
    "                # 그래프를 byte형식으로 읽어와 parsing해서 graph_def에 할당\n",
    "            input_image, output = tf.import_graph_def(graph_def, return_elements=['input:0', 'output:0'])\n",
    "            # parsing한 그래프를 default graph에 할당\n",
    "            # input, output tensor를 가져옴. input은 placeholder이고 output은 최종 결과물\n",
    "        else:\n",
    "            # 저장된 check point를 불러옴\n",
    "            if ckpt < 0:\n",
    "                checkpoint = tf.train.get_checkpoint_state('./ckpts/{}/'.format(style_name))\n",
    "                input_checkpoint = checkpoint.model_checkpoint_path\n",
    "                # 가장 최근의 checkpoint를 불러옴\n",
    "            else:\n",
    "                input_checkpoint = './ckpts/{}/{}-{}'.format(style_name, style_name, ckpt)\n",
    "                # 지정한 checkpoint를 불러옴\n",
    "            saver = tf.train.import_meta_graph(input_checkpoint + '.meta')\n",
    "            # 그래프를 불러와 갱신\n",
    "            saver.restore(sess, input_checkpoint)\n",
    "            # checkpoint의 파라미터를 세션에 갱신\n",
    "            graph = tf.get_default_graph()\n",
    "            # default graph에 접근\n",
    "            \n",
    "            input_image = graph.get_tensor_by_name('input:0')\n",
    "            output = graph.get_tensor_by_name('output:0')\n",
    "            # 그래프의 input, output tensor를 받아와 할당함\n",
    "        out = sess.run(output, feed_dict={input_image: shaped_input})\n",
    "        # 할당된 input tensor에 input image를 넣고 output tensor를 실행시켜 값을 얻음\n",
    "\n",
    "out = out.reshape((out.shape[1:]))\n",
    "# [224,224,3]의 형태로 조정\n",
    "im = Image.fromarray(np.uint8(out))\n",
    "\n",
    "im = im.resize(original_image.size, resample=Image.LANCZOS)\n",
    "# 조정했던 이미지를 다시 원래 사이즈로 변경\n",
    "im.save(outfile_path)\n",
    "# 이미지를 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"result1.png\" width=\"500px\">\n",
    "<img src=\"result2.png\" width=\"500px\">\n",
    "<img src=\"result3.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference : https://github.com/antlerros/tensorflow-fast-neuralstyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
